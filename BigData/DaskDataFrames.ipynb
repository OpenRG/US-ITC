{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data in Python with Dask DataFrames\n",
    "### by [Jason DeBacker](https://jasondebacker.com), October 2024\n",
    "\n",
    "In this notebook, we will explore how to use Dask DataFrames to work with big data in Python. Dask is a flexible parallel computing library for analytic computing. It is designed to scale from single machines to large clusters and it provides a familiar DataFrame interface that can scale to big data.\n",
    "\n",
    "Dask DataFrames are a high-level interface to distributed computing and can handle large datasets that don't fit into memory. Dask DataFrames are built on top of Pandas DataFrames and provide a similar API. This makes it easy to switch between Pandas and Dask DataFrames.  \n",
    "\n",
    "Note that Dask is not a panacea and there one wants to be aware of [when to use and not use Dask DataFrames](https://docs.dask.org/en/stable/dataframe.html#when-not-to-use-dask-dataframes).\n",
    "\n",
    "Files used in this notebook can be found in my Dropbox [here](https://www.dropbox.com/scl/fo/w24rc5qw87n8g6d1cm3nr/AFqZZELatCOF4Bryg6g7KSo?rlkey=f1hb8chhbd30cz6zdo48n90h9&st=b6f87a0o&dl=0).  Note files are up to 10GB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "# from memory_profiler import profile\n",
    "import memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to allow use of memit for memory profiling\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to data\n",
    "CUR_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(CUR_DIR, 'data')\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'trade_data_1962_to_2022.csv')\n",
    "DATA_FILE2 = os.path.join(DATA_DIR, 'sitc_country_country_product_year_4_2022.dta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read large data file into Pandas DataFrame\n",
    "start_time = time.time()\n",
    "trade_data_df = pd.read_csv(DATA_FILE)\n",
    "print(f\"Time taken to read data into Pandas DataFrame: {time.time() - start_time}\")\n",
    "# Read large datafile into Dask DataFrame\n",
    "start_time = time.time()\n",
    "trade_data_dd = dd.read_csv(DATA_FILE)\n",
    "print(f\"Time taken to read data into Dask DataFrame: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are these data?\n",
    "print(f\"Size of trade_data_df: {trade_data_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_dd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the below is running, check out dask DataFrames documentation: [https://docs.dask.org/en/stable/dataframe.html](https://docs.dask.org/en/stable/dataframe.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Dask client\n",
    "from dask.distributed import Client\n",
    "client = Client(n_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time how long to describe data with Pandas and Dask\n",
    "start_time = time.time()\n",
    "trade_data_df.describe()\n",
    "print(f\"Time taken to describe data with Pandas: {time.time() - start_time}\")\n",
    "start_time = time.time()\n",
    "trade_data_dd.describe().compute(client=client)\n",
    "print(f\"Time taken to describe data with Dask: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data_dd.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group data\n",
    "\n",
    "Here we apply the `groupby` operation to Dask and Pandas DataFrames to produce some plots.\n",
    "\n",
    "Note that Pandas is faster than Dask because the shuffling of data necessary for the `groupby` operation is slow when done across workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot with export volume by country over the years\n",
    "start_time = time.time()\n",
    "fig = trade_data_dd.groupby(['country_id'])['export_value'].sum().nlargest(10).compute().plot.bar()\n",
    "# read in csv to get country names\n",
    "iso_codes = pd.read_csv(\"https://gist.githubusercontent.com/radcliff/f09c0f88344a7fcef373/raw/2753c482ad091c54b1822288ad2e4811c021d8ec/wikipedia-iso-country-codes.csv\")\n",
    "# put names iso code to names in dictionary\n",
    "country_dict = iso_codes.set_index('Numeric code')['English short name lower case'].to_dict()\n",
    "# update x labels with country names\n",
    "fig.set_xticklabels(labels=[country_dict[int(x.get_text())] for x in fig.get_xticklabels()])\n",
    "print(f\"Time taken to plot data with Dask: {time.time() - start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot with export volume by country over the years\n",
    "start_time = time.time()\n",
    "fig = trade_data_df.groupby(['country_id'])['export_value'].sum().nlargest(10).plot.bar()\n",
    "# read in csv to get country names\n",
    "iso_codes = pd.read_csv(\"https://gist.githubusercontent.com/radcliff/f09c0f88344a7fcef373/raw/2753c482ad091c54b1822288ad2e4811c021d8ec/wikipedia-iso-country-codes.csv\")\n",
    "# put names iso code to names in dictionary\n",
    "country_dict = iso_codes.set_index('Numeric code')['English short name lower case'].to_dict()\n",
    "# update x labels with country names\n",
    "fig.set_xticklabels(labels=[country_dict[int(x.get_text())] for x in fig.get_xticklabels()])\n",
    "print(f\"Time taken to plot data with Pandas: {time.time() - start_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory profiling\n",
    "\n",
    "Below does some memory profiling to see how much memory is used to read in and merge data with Pandas and Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Merge example using pandas\n",
    "#########################################\n",
    "\n",
    "def pd_profile_func(trade_data):\n",
    "\n",
    "    # read in two data set to merge with pandas\n",
    "    # trade_data = pd.read_stata(DATA_FILE2)\n",
    "\n",
    "    # create partner data with eci, coi for partner\n",
    "    partner_data = trade_data[['country_id', 'product_id', 'year', 'eci', 'coi']]\n",
    "\n",
    "    # merge in pandas\n",
    "    merged = trade_data.merge(right=partner_data, left_on=\"partner_country_id\", right_on=\"country_id\")\n",
    "\n",
    "    # write the merged data out\n",
    "    merged.to_csv(os.path.join(DATA_DIR, \"trade_merged_pd.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata(DATA_FILE2, nrows=300_000)\n",
    "%memit pd_profile_func(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Merge example using dask\n",
    "#########################################\n",
    "\n",
    "def dd_profile_func(trade_data):\n",
    "\n",
    "    # read in data set to merge with dask\n",
    "    # trade_data = dd.read_csv(DATA_FILE2)\n",
    "\n",
    "    # create partner data with eci, coi for partner\n",
    "    partner_data = trade_data[['country_id', 'product_id', 'year', 'eci', 'coi']]\n",
    "    print(type(trade_data))\n",
    "    print(type(partner_data))\n",
    "\n",
    "    # merge in dask\n",
    "    merged_dd = trade_data.merge(right=partner_data, left_on=\"partner_country_id\", right_on=\"country_id\")\n",
    "\n",
    "    # write the merged data out\n",
    "    merged_dd.to_csv(os.path.join(DATA_DIR, \"trade_merged_dd.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(pd.read_stata(DATA_FILE2, nrows=300_000))\n",
    "%memit dd_profile_func(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pandas-mem-profile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above may take a while to run. Here's the output:\n",
    "```\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "     8    122.5 MiB    122.5 MiB           1   @profile\n",
    "     9                                         def profile_func():\n",
    "    10                                         \n",
    "    11                                             # packages\n",
    "    12    122.5 MiB      0.0 MiB           1       CUR_DIR = os.getcwd()\n",
    "    13    122.5 MiB      0.0 MiB           1       DATA_DIR = os.path.join(CUR_DIR, 'data')\n",
    "    14                                         \n",
    "    15                                             # read in two data set to merge with pandas\n",
    "    16                                             # trade_data = pd.read_csv(os.path.join(DATA_DIR, \"trade_data_1962_to_2022.csv\"))\n",
    "    17    497.0 MiB    374.4 MiB           1       trade_data = pd.read_stata(os.path.join(DATA_DIR, \"sitc_country_country_product_year_4_2022.dta\"))\n",
    "    18                                             # # keep if year > 2021\n",
    "    19                                             # trade_data = trade_data[trade_data['year'] > 2021]\n",
    "    20                                             # keep just first 1000 rows\n",
    "    21    497.0 MiB      0.0 MiB           1       trade_data = trade_data.head(100_000)\n",
    "    22    497.0 MiB      0.0 MiB           1       print(\"Num obs = \", len(trade_data))\n",
    "    23                                         \n",
    "    24                                             # create partner data with eci, coi for partner\n",
    "    25    498.9 MiB      1.9 MiB           1       partner_data = trade_data[['country_id', 'product_id', 'year', 'eci', 'coi']]\n",
    "    26                                         \n",
    "    27                                             # merge in pandas\n",
    "    28   6258.9 MiB   5760.0 MiB           1       merged = trade_data.merge(right=partner_data, left_on=\"partner_country_id\", right_on=\"country_id\")\n",
    "    29                                         \n",
    "    30                                             # write the merged data out\n",
    "    31   6295.3 MiB     36.4 MiB           1       merged.to_csv(os.path.join(DATA_DIR, \"trade_merged_pd.csv\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dask-mem-profile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above may take a while to run. Here's the output:\n",
    "```\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "     9    163.8 MiB    163.8 MiB           1   @profile\n",
    "    10                                         def profile_func():\n",
    "    11\n",
    "    12                                             # packages\n",
    "    13    163.8 MiB      0.0 MiB           1       CUR_DIR = os.getcwd()\n",
    "    14    163.8 MiB      0.0 MiB           1       DATA_DIR = os.path.join(CUR_DIR, 'data')\n",
    "    15\n",
    "    16                                             # read in two data set to merge with pandas\n",
    "    17    166.3 MiB      2.4 MiB           1       trade_data = dd.read_csv(os.path.join(DATA_DIR, \"trade_data_2022.csv\"))\n",
    "    18                                             # keep just first rows\n",
    "    19    584.5 MiB    418.2 MiB           1       trade_data = trade_data.head(100_000)\n",
    "    20    584.5 MiB      0.0 MiB           1       print(\"Num obs = \", len(trade_data))\n",
    "    21\n",
    "    22                                             # create partner data with eci, coi for partner\n",
    "    23    584.7 MiB      0.2 MiB           1       partner_data = trade_data[['country_id', 'product_id', 'year', 'eci', 'coi']]\n",
    "    24\n",
    "    25                                             # merge in pandas\n",
    "    26  10126.3 MiB   9541.6 MiB           1       merged = trade_data.merge(right=partner_data, left_on=\"partner_country_id\", right_on=\"country_id\")\n",
    "    27\n",
    "    28                                             # write the merged data out\n",
    "    29  10158.1 MiB     31.8 MiB           1       merged.to_csv(os.path.join(DATA_DIR, \"trade_merged_pd.csv\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge on index\n",
    "\n",
    "Merges on index are faster than merges on columns.  When possible, set the index of the DataFrames to be merged on to the same column and merge on index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to merge data with Pandas merge: 0.0056188106536865234\n",
      "Time taken to merge data with Pandas merge on index: 0.0007998943328857422\n"
     ]
    }
   ],
   "source": [
    "trade_data = pd.read_csv(os.path.join(DATA_DIR, \"trade_data_2022.csv\"))\n",
    "# drop duplicates of country_id\n",
    "trade_data.drop_duplicates(subset=\"country_id\", inplace=True)  ## data is by country-product -- but eci only vary by country\n",
    "# create partner data with eci, coi for partner\n",
    "partner_data = trade_data[['country_id', 'eci']].copy()\n",
    "# drop duplicates\n",
    "partner_data.drop_duplicates(inplace=True)\n",
    "# merge in pandas\n",
    "start_time = time.time()\n",
    "merged = trade_data.merge(right=partner_data, left_on=\"partner_country_id\", right_on=\"country_id\", how=\"left\")\n",
    "print(f\"Time taken to merge data with Pandas merge: {time.time() - start_time}\")\n",
    "\n",
    "# set country_id as index\n",
    "trade_data2 = trade_data.copy()\n",
    "partner_data2 = partner_data.copy()\n",
    "trade_data2.set_index(\"partner_country_id\", inplace=True)\n",
    "partner_data2.set_index(\"country_id\", inplace=True)\n",
    "start_time = time.time()\n",
    "merged = trade_data2.merge(partner_data2, left_index=True, right_index=True, how=\"left\")\n",
    "print(f\"Time taken to merge data with Pandas merge on index: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 ms ± 638 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "176 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit merged = trade_data.merge(right=partner_data, left_on=\"partner_country_id\", right_on=\"country_id\", how=\"left\")\n",
    "%timeit merged = trade_data2.merge(partner_data2, left_index=True, right_index=True, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up files saved for this exercise\n",
    "os.remove(os.path.join(DATA_DIR, \"trade_merged_pd.csv\"))\n",
    "os.remove(os.path.join(DATA_DIR, \"trade_merged_dd.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usitc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques for Handling Big Data in Python\n",
    "### by [Jason DeBacker](https://jasondebacker.com), October 2024\n",
    "\n",
    "This notebook offers some techniques for handling big data in Python.  The techniques include:\n",
    "* Chunking data\n",
    "* Reading and testing code with a sample of data\n",
    "* Using efficient data types\n",
    "* `pyreadstat` for reading SAS/Stata/SPSS files: [https://github.com/Roche/pyreadstat](https://github.com/Roche/pyreadstat)\n",
    "* SQL in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyreadstat\n",
    "import os\n",
    "# from memory_profiler import profile\n",
    "import memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "# to allow use of memit for memory profiling\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to data\n",
    "CUR_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(CUR_DIR, 'data')\n",
    "DATA_FILE = os.path.join(DATA_DIR, 'trade_data_1962_to_2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Pandas More Efficient\n",
    "\n",
    "#### 1. Test code on subset of data\n",
    "\n",
    "Use the `nrows` argument in the Pandas `read_XXX` command (note, not all read commands have this option) to read in only a subset of the data.  This is useful for testing code on a smaller subset of the data before running on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_FILE, nrows=10_000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make more efficient data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_id              int64\n",
       "partner_country_id      int64\n",
       "year                    int64\n",
       "product_id              int64\n",
       "export_value          float64\n",
       "import_value          float64\n",
       "coi                   float64\n",
       "eci                   float64\n",
       "pci                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see datatypes for each column\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                   132\n",
       "country_id            80000\n",
       "partner_country_id    80000\n",
       "year                  80000\n",
       "product_id            80000\n",
       "export_value          80000\n",
       "import_value          80000\n",
       "coi                   80000\n",
       "eci                   80000\n",
       "pci                   80000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage(deep=True)  # memory usage in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_id             uint16\n",
       "partner_country_id     uint16\n",
       "year                   uint16\n",
       "product_id             uint16\n",
       "export_value          float32\n",
       "import_value          float32\n",
       "coi                   float32\n",
       "eci                   float32\n",
       "pci                   float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "df2[[\"country_id\", \"partner_country_id\", \"product_id\", \"year\"]] = (\n",
    "    df2[[\"country_id\", \"partner_country_id\", \"product_id\", \"year\"]].apply(pd.to_numeric, downcast=\"unsigned\")\n",
    ")\n",
    "df2[[\"export_value\", \"import_value\", \"coi\", \"eci\", \"pci\"]] = (\n",
    "    df2[[\"export_value\", \"import_value\", \"coi\", \"eci\", \"pci\"]].apply(pd.to_numeric, downcast=\"float\")\n",
    ")\n",
    "\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                   132\n",
       "country_id            20000\n",
       "partner_country_id    20000\n",
       "year                  20000\n",
       "product_id            20000\n",
       "export_value          40000\n",
       "import_value          40000\n",
       "coi                   40000\n",
       "eci                   40000\n",
       "pci                   40000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39\n"
     ]
    }
   ],
   "source": [
    "reduction = df2.memory_usage(deep=True).sum() / df.memory_usage(deep=True).sum()\n",
    "print(f\"{reduction:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing a few data types, we reduce the memory usage by 61%.  This can be a big deal when working with big data.  And with strings, you can use the `category` data type in Pandas to reduce memory usage, sometimes very significantly.\n",
    "\n",
    "However, be careful with making sure you don't loose important precision of extreme values by downcasting the data types.  We can check by doing a `df.describe()` before and after downcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_id</th>\n",
       "      <th>partner_country_id</th>\n",
       "      <th>year</th>\n",
       "      <th>product_id</th>\n",
       "      <th>export_value</th>\n",
       "      <th>import_value</th>\n",
       "      <th>coi</th>\n",
       "      <th>eci</th>\n",
       "      <th>pci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006275</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>-7.187206e-09</td>\n",
       "      <td>5.327284e-08</td>\n",
       "      <td>3.308717e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>-0.024651</td>\n",
       "      <td>-1.520906e-08</td>\n",
       "      <td>-4.549485e-08</td>\n",
       "      <td>-2.185737e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.472961e-08</td>\n",
       "      <td>2.032776e-08</td>\n",
       "      <td>9.503174e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.795456e-09</td>\n",
       "      <td>2.289963e-10</td>\n",
       "      <td>2.861786e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.164053e-11</td>\n",
       "      <td>2.134323e-08</td>\n",
       "      <td>2.834473e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.077027e-09</td>\n",
       "      <td>-3.067474e-08</td>\n",
       "      <td>1.314545e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.889221e-08</td>\n",
       "      <td>-3.009033e-08</td>\n",
       "      <td>1.314545e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country_id  partner_country_id  year  product_id  export_value  \\\n",
       "count         0.0                 0.0   0.0         0.0      0.000000   \n",
       "mean          0.0                 0.0   0.0         0.0     -0.006275   \n",
       "std           0.0                 0.0   0.0         0.0     -0.000410   \n",
       "min           0.0                 0.0   0.0         0.0      0.000000   \n",
       "25%           0.0                 0.0   0.0         0.0      0.000000   \n",
       "50%           0.0                 0.0   0.0         0.0      0.000000   \n",
       "75%           0.0                 0.0   0.0         0.0      0.000000   \n",
       "max           0.0                 0.0   0.0         0.0      0.000000   \n",
       "\n",
       "       import_value           coi           eci           pci  \n",
       "count      0.000000  0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "mean       0.008750 -7.187206e-09  5.327284e-08  3.308717e-08  \n",
       "std       -0.024651 -1.520906e-08 -4.549485e-08 -2.185737e-08  \n",
       "min        0.000000 -4.472961e-08  2.032776e-08  9.503174e-09  \n",
       "25%        0.000000 -4.795456e-09  2.289963e-10  2.861786e-08  \n",
       "50%        0.000000  3.164053e-11  2.134323e-08  2.834473e-08  \n",
       "75%        0.000000  7.077027e-09 -3.067474e-08  1.314545e-09  \n",
       "max        0.000000  9.889221e-08 -3.009033e-08  1.314545e-09  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.describe() - df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chunking data\n",
    "\n",
    "Pandas has some built-in options to read data in \"chunks\", so you don't have to store it all in memory at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data():\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "    mean_exports = df[\"export_value\"].mean()\n",
    "    print(f\"Mean exports: {mean_exports:,.2f}\")\n",
    "\n",
    "def chunked_data():\n",
    "    chunksize = 100_000\n",
    "    total = 0\n",
    "    count = 0\n",
    "    df = pd.read_csv(DATA_FILE, chunksize=chunksize)  # NOTE: using the chunksize argument returns an iterable list of dataframes\n",
    "    for chunk in pd.read_csv(DATA_FILE, chunksize=chunksize):\n",
    "        total += chunk[\"export_value\"].sum()\n",
    "        count += chunk[\"export_value\"].count()\n",
    "    print(f\"Mean exports: {total/count:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean exports: 2,589,223.40\n",
      "peak memory: 25005.86 MiB, increment: 23400.36 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit full_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean exports: 2,589,223.40\n",
      "peak memory: 2982.42 MiB, increment: 49.64 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit chunked_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiently reading SAS, STATA, SPSS files\n",
    "\n",
    "The `pyreadstat` package is a Python package that allows you to read SAS, Stata, and SPSS files into Pandas DataFrames very efficiently.  It is a wrapper around the `Readstat` C library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHID</th>\n",
       "      <th>year</th>\n",
       "      <th>cu18</th>\n",
       "      <th>kids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000002</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000002</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HHID    year  cu18  kids\n",
       "0  000001  1992.0   0.0   4.0\n",
       "1  000001  1994.0   0.0   4.0\n",
       "2  000002  1992.0   0.0   8.0\n",
       "3  000002  1994.0   0.0   8.0\n",
       "4  000002  1996.0   0.0   8.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, meta = pyreadstat.read_sas7bdat(os.path.join(DATA_DIR, \"child_u18.sas7bdat\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.66 s ± 840 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "282 ms ± 42.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Horse race between pandas and pyreadstat, reading a largish Stata file\n",
    "%timeit df, meta = pyreadstat.read_dta(os.path.join(DATA_DIR, \"sitc_country_country_product_year_4_2022.dta\"))\n",
    "%timeit df = pd.read_stata(os.path.join(DATA_DIR, \"sitc_country_country_product_year_4_2022.dta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 ms ± 27.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "40.1 ms ± 3.85 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df, meta = pyreadstat.read_sas7bdat(os.path.join(DATA_DIR, \"child_u18.sas7bdat\"))\n",
    "%timeit df = pd.read_sas(os.path.join(DATA_DIR, \"child_u18.sas7bdat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other advantages of pyreadstat\n",
    "1. Read only a subset of columns (can't do this on SAS or STATA files in pd.read_sas or pd.read_stata)\n",
    "2. Built in mulitprocessing\n",
    "3. Can read metadata only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHID</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>1992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>1994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002</td>\n",
       "      <td>1992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000002</td>\n",
       "      <td>1994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000002</td>\n",
       "      <td>1996.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     HHID    year\n",
       "0  000001  1992.0\n",
       "1  000001  1994.0\n",
       "2  000002  1992.0\n",
       "3  000002  1994.0\n",
       "4  000002  1996.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. Read only a subset of columns\n",
    "df, meta = pyreadstat.read_sas7bdat(os.path.join(DATA_DIR, \"child_u18.sas7bdat\"), usecols=[\"HHID\", \"year\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.02 s ± 133 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "5.15 s ± 343 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# 2. Built in multiprocessing\n",
    "%timeit df, meta = pyreadstat.read_dta(os.path.join(DATA_DIR, \"sitc_country_country_product_year_4_2022.dta\"))\n",
    "%timeit df, meta = pyreadstat.read_file_multiprocessing(pyreadstat.read_dta, os.path.join(DATA_DIR, \"sitc_country_country_product_year_4_2022.dta\"), num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HHID', 'year', 'cu18', 'kids']\n"
     ]
    }
   ],
   "source": [
    "# Read metadata only\n",
    "_, meta = pyreadstat.read_sas7bdat(os.path.join(DATA_DIR, \"child_u18.sas7bdat\"), metadataonly=True)  # note, still returns DF\n",
    "print(meta.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NONWORKING IN THIS NOTEBOOK) Example of SQL in Python\n",
    "# Connect to WRDS (Wharton Research Data Services) using the wrds package\n",
    "import wrds\n",
    "import pandas as pd\n",
    "import os\n",
    "db = wrds.Connection(wrds_username='jdebacker')\n",
    "\n",
    "\n",
    "# Retrieve 2015 - 2017 Earnings Conference Call Transcripts with Full-text components\n",
    "# loop over the companyid list\n",
    "for year in range(2015, 2018):\n",
    "    df_list = []\n",
    "    # Loop over the company ids for a given year\n",
    "    for companyid in companyid_list:\n",
    "        sql_query = (\n",
    "            \"SELECT a.companyid, a.headline, a.companyname, a.mostimportantdateutc, \"\n",
    "            \"a.mostimportanttimeutc, b.componenttext, b.transcriptComponentId, \"\n",
    "            \"b.componentOrder, b.transcriptPersonId, b.transcriptId, \"\n",
    "            \"c.companyName, c.proId, c.speakerTypeId, d.speakerTypeName, \"\n",
    "            \"e.transcriptComponentTypeName \"\n",
    "            \"FROM ciq_transcripts.wrds_transcript_detail AS a \"\n",
    "            \"JOIN ciq_transcripts.ciqtranscriptcomponent as b ON a.transcriptid=b.transcriptid \"\n",
    "            \"JOIN ciq_transcripts.ciqtranscriptperson as c ON b.transcriptPersonId=c.transcriptPersonID \"\n",
    "            \"JOIN ciq_transcripts.ciqtranscriptspeakertype as d ON c.speakerTypeId=d.speakerTypeId \"\n",
    "            \"JOIN ciq_transcripts.ciqtranscriptcomponenttype as e ON  b.transcriptComponentTypeId=e.transcriptComponentTypeId \"\n",
    "            \"WHERE a.companyid={0} AND \"\n",
    "            \"date_part('year',mostimportantdateutc)={1}; \"\n",
    "        ).format(companyid, year)\n",
    "\n",
    "        print(\n",
    "            \"Running query for year {y} and companyid {c}\".format(\n",
    "                y=year, c=companyid\n",
    "            )\n",
    "        )\n",
    "        data = db.raw_sql(sql_query)\n",
    "        # drop duplicate observations of text components\n",
    "        data.drop_duplicates(\n",
    "            subset=[\n",
    "                \"componentorder\",\n",
    "                \"proid\",\n",
    "                \"headline\",\n",
    "                \"mostimportanttimeutc\",\n",
    "                \"mostimportantdateutc\",\n",
    "                \"transcriptcomponentid\",\n",
    "                \"componenttext\",\n",
    "            ],\n",
    "            keep=\"first\",\n",
    "            inplace=True,\n",
    "            ignore_index=False,\n",
    "        )\n",
    "        # append the data for a given company id to the list of dataframes\n",
    "        df_list.append(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usitc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
